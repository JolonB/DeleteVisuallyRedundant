#!/usr/bin/env python3

import argparse
import os
import re
from typing import List

DUPLICATES_FILE = "dups.txt"
FILENAME_REGEX = re.compile("(.+?\.(?:jpe?g|png|gif))(?:\s+|$)", flags=re.IGNORECASE)

parser = argparse.ArgumentParser(
    prog="DeleteVisuallyRedundant",
    description="Delete visually similar images. Retain only the largest and oldest image.",
)
parser.add_argument(
    "directory",
    help="The directory containing the images"
)
parser.add_argument(
    "-r",
    "--retain",
    action="store_true",
    help="Retain the dups.txt file generated by findimagedups",
)
parser.add_argument(
    "-d",
    "--dry-run",
    action="store_true",
    help="Print the names of files without actually deleting them",
)


def find_duplicates(directory:str):
    """Find duplicate files from within the provided directory using findimagedupes.

    Args:
        directory (str): The directory to search within.
    """
    # TODO can this be run using Python instead of system calls?
    os.system('findimagedupes -R "{}" > "{}"'.format(directory, DUPLICATES_FILE))


def delete_all_but_original(filepaths:List, dry_run:bool=False):
    """From a list of files, delete any files that are not the largest.
    If any files remain, delete any that are not the oldest.
    If any files still remain, delete all but one of them.

    Args:
        filepaths (List): A list of paths to the files.
        dry_run (bool, optional): Only print filenames without deleting. Defaults to False.
    """
    def delete_file(filepath:str):
        """Print the name of the given file and delete it if dry_run is False.

        Args:
            filepath (str): The path to the file.
        """
        print("D: {}".format(filepath))
        if not dry_run:
            os.remove(filepath)

    # Keep track of any files that weren't deleted
    largest_files = []
    remaining_files = []

    # Find the size of the largest file
    max_size = max(os.stat(filepath).st_size for filepath in filepaths)
    # Delete any files that are smaller
    for filepath in filepaths:
        size = os.stat(filepath).st_size

        if size < max_size:
            delete_file(filepath)
        elif size == max_size:
            largest_files.append(filepath)

    if len(largest_files) > 1:
        # Get the earliest modified file
        oldest_modified_time = min(
            os.stat(filepath).st_mtime for filepath in largest_files
        )
        # Delete any files that are more recent
        for filepath in largest_files:
            modified_time = os.stat(filepath).st_mtime

            if modified_time > oldest_modified_time:
                delete_file(filepath)
            elif modified_time == oldest_modified_time:
                remaining_files.append(filepath)

        # Remove all but one duplicate if any still exist
        for filepath in remaining_files[1:]:
            delete_file(filepath)


def main(directory:str, keep_temp_file:bool=False, dry_run:bool=False):
    """Find all duplicate files in the provided directory. Delete any duplicate files.

    Args:
        directory (str): The directory to look within.
        keep_temp_file (bool, optional): Keep the temporary file generated when finding duplicates. Defaults to False.
        dry_run (bool, optional): Don't delete any files. Only report files that would be deleted. Defaults to False.
    """
    find_duplicates(directory)
    with open(DUPLICATES_FILE, "r") as fp:
        for line in fp:
            matches = FILENAME_REGEX.findall(line)

            delete_all_but_original(matches, dry_run=dry_run)

    if not keep_temp_file:
        os.remove(DUPLICATES_FILE)


if __name__ == "__main__":
    args = parser.parse_args()
    directory = args.directory
    keep_temp_file = args.retain
    dry_run = args.dry_run

    main(directory, keep_temp_file=keep_temp_file, dry_run=dry_run)
