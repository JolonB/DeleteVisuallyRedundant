#!/usr/bin/env python3

import argparse
import os
import re

DUPLICATES_FILE = "dups.txt"
FILENAME_REGEX = re.compile("(.+?\.(?:jpe?g|png|gif))(?:\s+|$)", flags=re.IGNORECASE)

parser = argparse.ArgumentParser(
    prog="DeleteVisuallyRedundant",
    description="Delete visually similar images. Retain only the largest and oldest image.",
)
parser.add_argument("filepath")
parser.add_argument(
    "-r",
    "--retain",
    action="store_true",
    help="Retain the dups.txt file generated by findimagedups",
)
parser.add_argument(
    "-d",
    "--dry-run",
    action="store_true",
    help="Print the names of files without actually deleting them",
)


def find_duplicates(filepath):
    # TODO can this be run using Python instead of system calls?
    os.system('findimagedupes -R "{}" > "{}"'.format(filepath, DUPLICATES_FILE))


def delete_all_but_original(filepaths, dry_run=False):
    # TODO add comments!!
    def delete_file(filepath):
        print("D: {}".format(filepath))
        if not dry_run:
            os.remove(filepath)

    # Keep track of any files that weren't deleted
    largest_files = []
    remaining_files = []

    # Find the size of the largest file
    max_size = max(os.stat(filepath).st_size for filepath in filepaths)
    # Delete any files that are smaller
    for filepath in filepaths:
        size = os.stat(filepath).st_size

        if size < max_size:
            delete_file(filepath)
        elif size == max_size:
            largest_files.append(filepath)

    if len(largest_files) > 1:
        # Get the earliest modified file
        oldest_modified_time = min(
            os.stat(filepath).st_mtime for filepath in largest_files
        )
        # Delete any files that are more recent
        for filepath in largest_files:
            modified_time = os.stat(filepath).st_mtime

            if modified_time > oldest_modified_time:
                delete_file(filepath)
            elif modified_time == oldest_modified_time:
                remaining_files.append(filepath)

        # Remove all but one duplicate if any still exist
        for filepath in remaining_files[1:]:
            delete_file(filepath)


def main(filepath, keep_temp_file=False, dry_run=False):
    find_duplicates(filepath)
    with open(DUPLICATES_FILE, "r") as fp:
        for line in fp:
            matches = FILENAME_REGEX.findall(line)

            delete_all_but_original(matches, dry_run=dry_run)

    if not keep_temp_file:
        os.remove(DUPLICATES_FILE)


if __name__ == "__main__":
    args = parser.parse_args()
    filepath = args.filepath
    keep_temp_file = args.retain
    dry_run = args.dry_run

    main(filepath, keep_temp_file=keep_temp_file, dry_run=dry_run)
